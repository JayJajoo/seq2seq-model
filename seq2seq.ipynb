{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T20:14:53.537242Z",
     "iopub.status.busy": "2025-04-09T20:14:53.536798Z",
     "iopub.status.idle": "2025-04-09T20:14:53.545159Z",
     "shell.execute_reply": "2025-04-09T20:14:53.543639Z",
     "shell.execute_reply.started": "2025-04-09T20:14:53.537193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "import kagglehub\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import pickle\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"devicharith/language-translation-englishfrench\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:03.968492Z",
     "iopub.status.busy": "2025-04-09T19:28:03.968194Z",
     "iopub.status.idle": "2025-04-09T19:28:04.429170Z",
     "shell.execute_reply": "2025-04-09T19:28:04.428194Z",
     "shell.execute_reply.started": "2025-04-09T19:28:03.968461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/kaggle/input/language-translation-englishfrench/eng_-french.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:04.431996Z",
     "iopub.status.busy": "2025-04-09T19:28:04.431636Z",
     "iopub.status.idle": "2025-04-09T19:28:04.462390Z",
     "shell.execute_reply": "2025-04-09T19:28:04.461528Z",
     "shell.execute_reply.started": "2025-04-09T19:28:04.431963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:04.463933Z",
     "iopub.status.busy": "2025-04-09T19:28:04.463623Z",
     "iopub.status.idle": "2025-04-09T19:28:04.472806Z",
     "shell.execute_reply": "2025-04-09T19:28:04.471936Z",
     "shell.execute_reply.started": "2025-04-09T19:28:04.463911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset  \n",
    "        self.eng_wtoi, self.eng_itow = self.build_vocab(self.data.iloc[:, 0])  # English column\n",
    "        self.fr_wtoi, self.fr_itow = self.build_vocab(self.data.iloc[:, 1])   # French column\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"([^\\w\\s])\", r\" \\1 \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return [\"<sos>\"] + text.split() + [\"<eos>\"]\n",
    "\n",
    "    def build_vocab(self, series, min_freq=1):\n",
    "        counter = Counter()\n",
    "        for text in series:\n",
    "            tokens = self.preprocess_text(text)\n",
    "            counter.update(tokens)\n",
    "        vocab = [\"<pad>\",\"<unk>\"]\n",
    "        vocab += list(counter.keys())\n",
    "        wtoi = {word: idx for idx, word in enumerate(vocab)}\n",
    "        itow = {idx: word for word, idx in wtoi.items()}\n",
    "        return wtoi, itow\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        eng_sentence = self.preprocess_text(self.data.iloc[index][0])\n",
    "        fr_sentence = self.preprocess_text(self.data.iloc[index][1])\n",
    "\n",
    "        eng_indices = [self.eng_wtoi.get(token, self.eng_wtoi[\"<unk>\"]) for token in eng_sentence]\n",
    "        fr_indices = [self.fr_wtoi.get(token, self.fr_wtoi[\"<unk>\"]) for token in fr_sentence]\n",
    "\n",
    "        return torch.tensor(eng_indices), torch.tensor(fr_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_function(batch, pad_idx_eng, pad_idx_fr):\n",
    "    sources, targets = zip(*batch)\n",
    "    padded_sources = pad_sequence(sources, batch_first=True, padding_value=pad_idx_eng)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_idx_fr)\n",
    "\n",
    "    return padded_sources, padded_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:04.474010Z",
     "iopub.status.busy": "2025-04-09T19:28:04.473629Z",
     "iopub.status.idle": "2025-04-09T19:28:08.472682Z",
     "shell.execute_reply": "2025-04-09T19:28:08.471729Z",
     "shell.execute_reply.started": "2025-04-09T19:28:04.473987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(dataset)\n",
    "PAD_IDX_ENG = custom_dataset.eng_wtoi[\"<pad>\"]\n",
    "PAD_IDX_FR = custom_dataset.fr_wtoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:08.473945Z",
     "iopub.status.busy": "2025-04-09T19:28:08.473597Z",
     "iopub.status.idle": "2025-04-09T19:28:08.506836Z",
     "shell.execute_reply": "2025-04-09T19:28:08.505983Z",
     "shell.execute_reply.started": "2025-04-09T19:28:08.473911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset,val_dataset,test_dataset = random_split(custom_dataset,[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:08.508047Z",
     "iopub.status.busy": "2025-04-09T19:28:08.507753Z",
     "iopub.status.idle": "2025-04-09T19:28:08.531390Z",
     "shell.execute_reply": "2025-04-09T19:28:08.530573Z",
     "shell.execute_reply.started": "2025-04-09T19:28:08.508025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(train_dataset)+len(val_dataset)+len(test_dataset),len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:08.532542Z",
     "iopub.status.busy": "2025-04-09T19:28:08.532314Z",
     "iopub.status.idle": "2025-04-09T19:28:08.550405Z",
     "shell.execute_reply": "2025-04-09T19:28:08.549793Z",
     "shell.execute_reply.started": "2025-04-09T19:28:08.532522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset,batch_size = batch_size, collate_fn=lambda batch: collate_function(batch, PAD_IDX_ENG, PAD_IDX_FR))\n",
    "val_dataloader = DataLoader(val_dataset,batch_size = batch_size, collate_fn=lambda batch: collate_function(batch, PAD_IDX_ENG, PAD_IDX_FR))\n",
    "test_dataloader = DataLoader(test_dataset,batch_size = batch_size, collate_fn=lambda batch: collate_function(batch, PAD_IDX_ENG, PAD_IDX_FR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:28:08.551597Z",
     "iopub.status.busy": "2025-04-09T19:28:08.551219Z",
     "iopub.status.idle": "2025-04-09T19:28:08.579993Z",
     "shell.execute_reply": "2025-04-09T19:28:08.579175Z",
     "shell.execute_reply.started": "2025-04-09T19:28:08.551547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,emb_dim,hidden_size,num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=True,\n",
    "                            batch_first=True)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,batch):\n",
    "        batch=batch.to(self.device)\n",
    "        _,(hn,cn) = self.lstm(batch)\n",
    "        return hn,cn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,emb_size,hidden_size,output_size,num_layers,batch_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=emb_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=True,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, input_seq, hidden, cell):\n",
    "        input_seq = input_seq.to(self.device)\n",
    "        hidden, cell = hidden.to(self.device), cell.to(self.device)\n",
    "        lstm_out, (hidden, cell) = self.lstm(input_seq, (hidden, cell))\n",
    "        output = self.fc(lstm_out)\n",
    "        return output, hidden, cell\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self,en_vocab_size,fr_vocab_size,emb_dim,encoder,decoder,learning_rate,num_epochs,TRG_PAD_IDX):\n",
    "        super().__init__()\n",
    "        self.en_vocab_size = en_vocab_size\n",
    "        self.fr_vocab_size = fr_vocab_size\n",
    "        self.en_embedding = nn.Embedding(en_vocab_size,emb_dim)\n",
    "        self.fr_embedding = nn.Embedding(fr_vocab_size,emb_dim)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=learning_rate)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.teacher_forcing_ratio = 0.75\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def fit(self, train_data, val_data=None):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "            prev_val = 0.0\n",
    "            val_inc_count = 0\n",
    "            for sources, targets in train_data:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                sources, targets = sources.to(self.device), targets.to(self.device)\n",
    "                source_batch = self.en_embedding(sources)\n",
    "                target_batch = self.fr_embedding(targets)\n",
    "    \n",
    "                hn, cn = self.encoder(source_batch)\n",
    "\n",
    "                target_batch_starting = target_batch[:,0,:]\n",
    "                seq_length = targets.shape[1]\n",
    "                inputs = target_batch_starting\n",
    "                generated_outputs = []\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "                for i in range(1,seq_length):\n",
    "                    decoder_lstm_output , hn , cn = self.decoder.forward(inputs,hn,cn)\n",
    "                    use_teacher = random.random() < self.teacher_forcing_ratio\n",
    "                    if use_teacher:\n",
    "                        inputs = target_batch[:, i, :]  \n",
    "                        inputs = inputs.unsqueeze(1)\n",
    "                    else:\n",
    "                        inputs = decoder_lstm_output.argmax(dim=-1)  \n",
    "                        inputs = self.fr_embedding(inputs)      \n",
    "                    \n",
    "                    generated_outputs.append(decoder_lstm_output)\n",
    "                \n",
    "                generated_outputs = torch.stack(generated_outputs)\n",
    "                generated_outputs = generated_outputs.permute(2,1,0,3).squeeze(0)\n",
    "                generated_outputs = generated_outputs.contiguous().view(-1,self.fr_vocab_size)\n",
    "                expected_outputs = targets[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                loss = self.loss_fn(generated_outputs, expected_outputs)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            val_loss = 0.0\n",
    "            if val_data:\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    for sources, targets in val_data:\n",
    "                        sources, targets = sources.to(self.device), targets.to(self.device)\n",
    "                        source_batch = self.en_embedding(sources)\n",
    "                        target_batch = self.fr_embedding(targets)\n",
    "    \n",
    "                        hn, cn = self.encoder(source_batch)\n",
    "                        target_batch_starting = target_batch[:,0,:]\n",
    "                        seq_length = targets.shape[1]\n",
    "                        inputs = target_batch_starting\n",
    "                        generated_outputs = []\n",
    "                        for i in range(1,seq_length):\n",
    "                            decoder_lstm_output , hn , cn = self.decoder.forward(inputs.unsqueeze(1),hn,cn)\n",
    "                            inputs = target_batch[:,i,:]\n",
    "                            generated_outputs.append(decoder_lstm_output)\n",
    "                        generated_outputs = torch.stack(generated_outputs)\n",
    "                        generated_outputs = generated_outputs.permute(2,1,0,3).squeeze(0)\n",
    "                        generated_outputs = generated_outputs.contiguous().view(-1,self.fr_vocab_size)\n",
    "                        expected_outputs = targets[:, 1:].contiguous().view(-1)\n",
    "                        \n",
    "                        loss = self.loss_fn(generated_outputs, expected_outputs)\n",
    "                        val_loss += loss.item()\n",
    "            \n",
    "            train_ppl = math.exp(train_loss)\n",
    "            if val_data:\n",
    "                val_ppl = math.exp(val_loss)\n",
    "                print(f\"Epoch {epoch} : Train Loss - {train_loss:.4f} | Val Loss - {val_loss:.4f} | Train PPL - {train_ppl:.4f} | Val PPL - {val_ppl:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1} : Train Loss - {train_loss / len(train_data):.4f} | Train PPL - {train_ppl:.4f}\")\n",
    "\n",
    "            if(val_loss / len(val_data)>prev_val):\n",
    "                val_inc_count += 1 \n",
    "                if(val_inc_count>=2):\n",
    "                    break\n",
    "            else:\n",
    "                val_inc_count = 0\n",
    "            prev_val = val_loss / len(val_data)\n",
    "\n",
    "            model_save_filename = f\"/kaggle/working/epoch_{epoch + 1}.pth\"\n",
    "            torch.save(self.state_dict(), model_save_filename)\n",
    "            print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T08:41:56.027090Z",
     "iopub.status.busy": "2025-04-09T08:41:56.026891Z",
     "iopub.status.idle": "2025-04-09T08:41:58.910231Z",
     "shell.execute_reply": "2025-04-09T08:41:58.909608Z",
     "shell.execute_reply.started": "2025-04-09T08:41:56.027073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emd_dim=512\n",
    "hidden_size = 1000\n",
    "num_layers = 2\n",
    "fr_vocab_size = len(custom_dataset.fr_wtoi)\n",
    "en_vocab_size = len(custom_dataset.eng_wtoi)\n",
    "lr = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "encoder = Encoder(emb_dim=emd_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
    "decoder = Decoder(emb_size=emd_dim, hidden_size=hidden_size, output_size=fr_vocab_size, num_layers=num_layers, batch_size=batch_size)\n",
    "model = seq2seq(    \n",
    "    en_vocab_size=en_vocab_size,\n",
    "    fr_vocab_size=fr_vocab_size,\n",
    "    emb_dim=emd_dim,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    learning_rate=lr,\n",
    "    num_epochs=num_epochs,\n",
    "    TRG_PAD_IDX=PAD_IDX_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T20:17:11.887528Z",
     "iopub.status.busy": "2025-04-09T20:17:11.887082Z",
     "iopub.status.idle": "2025-04-09T20:17:11.899100Z",
     "shell.execute_reply": "2025-04-09T20:17:11.897590Z",
     "shell.execute_reply.started": "2025-04-09T20:17:11.887498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train Loss - 3.8774 | Val Loss - 2.6443 | Train PPL - 48.2985 | Val PPL - 14.0736\n",
      "Model Saved\n",
      "Epoch 2 : Train Loss - 2.5138 | Val Loss - 2.0460 | Train PPL - 12.3518 | Val PPL - 7.7369\n",
      "Model Saved\n",
      "Epoch 3 : Train Loss - 1.8909 | Val Loss - 1.6933 | Train PPL - 6.6253 | Val PPL - 5.4374\n",
      "Model Saved\n",
      "Epoch 4 : Train Loss - 1.4645 | Val Loss - 1.5378 | Train PPL - 4.3254 | Val PPL - 4.6543\n",
      "Model Saved\n",
      "Epoch 5 : Train Loss - 1.1496 | Val Loss - 1.4609 | Train PPL - 3.1569 | Val PPL - 4.3098\n",
      "Model Saved\n",
      "Epoch 6 : Train Loss - 0.9311 | Val Loss - 1.4447 | Train PPL - 2.5373 | Val PPL - 4.2406\n",
      "Model Saved\n",
      "Epoch 7 : Train Loss - 0.7584 | Val Loss - 1.4396 | Train PPL - 2.1349 | Val PPL - 4.2190\n",
      "Model Saved\n",
      "Epoch 8 : Train Loss - 0.6270 | Val Loss - 1.4443 | Train PPL - 1.8720 | Val PPL - 4.2389\n",
      "Model Saved\n",
      "Epoch 9 : Train Loss - 0.5968 | Val Loss - 1.4548 | Train PPL - 1.8163 | Val PPL - 4.2836\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits, temperature=1.0):\n",
    "    logits = logits / temperature\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "\n",
    "def generate(custom_dataset, encoder, decoder, model, sentence, max_gen_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = custom_dataset.preprocess_text(sentence)\n",
    "    \n",
    "    eng_indices = [custom_dataset.eng_wtoi.get(token, custom_dataset.eng_wtoi[\"<unk>\"]) for token in tokens]\n",
    "    \n",
    "    source_tensor = torch.tensor(eng_indices).unsqueeze(0).to(model.device)  # (1, seq_len)\n",
    "    source_embeddings = model.en_embedding(source_tensor)  \n",
    "    hn,cn = encoder.forward(source_embeddings)\n",
    "    fr_sos_index = custom_dataset.fr_wtoi.get(\"<sos>\")\n",
    "    fr_sos_embd = model.fr_embedding(torch.tensor([[fr_sos_index]]).to(model.device))\n",
    "    print(fr_sos_embd.shape,\"\\n\")\n",
    "    len_count = 0\n",
    "    prev_embedding = fr_sos_embd\n",
    "    words = []\n",
    "    while(True):\n",
    "        logits,hn,cn = decoder.forward(prev_embedding,hn,cn)\n",
    "        logits = logits.argmax(dim=-1)\n",
    "        max_probable_word_index = sample_with_temperature(logits[0], temperature=0.7)\n",
    "        prev_embedding = model.fr_embedding(torch.tensor([[max_probable_word_index]]).to(model.device))\n",
    "        word = custom_dataset.fr_itow.get(max_probable_word_index,\"<unk>\")\n",
    "        if(word==\"<eos>\"):\n",
    "            break\n",
    "        words.append(word)\n",
    "        len_count+=1\n",
    "        if(len_count>=max_gen_len):\n",
    "            break\n",
    "    return \" \".join(words),len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generate(custom_dataset, encoder, decoder, model, \"i did see something.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:41:53.705387Z",
     "iopub.status.busy": "2025-04-09T19:41:53.705068Z",
     "iopub.status.idle": "2025-04-09T19:41:53.713162Z",
     "shell.execute_reply": "2025-04-09T19:41:53.712426Z",
     "shell.execute_reply.started": "2025-04-09T19:41:53.705364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference(model,encoder,decoder,dataloader):\n",
    "    results={\"english_sentence\":[],\n",
    "             \"original_translation\":[],\n",
    "             \"produced_translation\":[]}\n",
    "    \n",
    "    for sources,targets in dataloader:\n",
    "\n",
    "        target_sentences = [[custom_dataset.fr_itow.get(index.item()) for index in sentence] for sentence in targets]\n",
    "        source_sentences = [[custom_dataset.eng_itow.get(index.item()) for index in sentence] for sentence in sources]\n",
    "        results[\"original_translation\"].extend(target_sentences)\n",
    "        results[\"english_sentence\"].extend(source_sentences)\n",
    "            \n",
    "        source_embds = model.en_embedding(sources.to(model.device)).to(model.device)\n",
    "        target_embds = model.fr_embedding(targets.to(model.device)).to(model.device)\n",
    "    \n",
    "        hn,cn = encoder.forward(source_embds)\n",
    "        target_batch_starting = target_embds[:,0,:]\n",
    "        seq_length = target_embds.shape[1]\n",
    "        inputs = target_batch_starting\n",
    "        generated_outputs = []\n",
    "        \n",
    "        for i in range(1,seq_length):\n",
    "            decoder_lstm_output , hn , cn = decoder.forward(inputs.unsqueeze(1),hn,cn)\n",
    "            inputs = target_embds[:,i,:]\n",
    "            generated_outputs.append(decoder_lstm_output)\n",
    "        \n",
    "        generated_outputs = torch.stack(generated_outputs)\n",
    "        generated_outputs = generated_outputs.permute(2,1,0,3).squeeze(0)\n",
    "    \n",
    "        probable_indexes = generated_outputs.argmax(dim=-1)\n",
    "    \n",
    "        produced_sentences = []\n",
    "        for sentence in probable_indexes:\n",
    "            sntc = []\n",
    "            for word_index in sentence:\n",
    "                sntc.append(custom_dataset.fr_itow.get(word_index))\n",
    "            produced_sentences.append(sntc)\n",
    "        results[\"produced_translation\"].append(produced_sentences)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:49:28.195671Z",
     "iopub.status.busy": "2025-04-09T19:49:28.195378Z",
     "iopub.status.idle": "2025-04-09T19:49:28.527245Z",
     "shell.execute_reply": "2025-04-09T19:49:28.526273Z",
     "shell.execute_reply.started": "2025-04-09T19:49:28.195649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/seq2seq-results/my_dict.pkl\", \"rb\") as f:\n",
    "    my_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:52:45.508085Z",
     "iopub.status.busy": "2025-04-09T19:52:45.507703Z",
     "iopub.status.idle": "2025-04-09T19:52:45.513361Z",
     "shell.execute_reply": "2025-04-09T19:52:45.512381Z",
     "shell.execute_reply.started": "2025-04-09T19:52:45.508055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_and_join_sentences(result_dict):\n",
    "    cleaned_results = {}\n",
    "    remove_tokens = {\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"}\n",
    "\n",
    "    for key, sentences in result_dict.items():\n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            cleaned = [word for word in sentence if word not in remove_tokens]\n",
    "            cleaned_sentences.append(\" \".join(cleaned))\n",
    "        cleaned_results[key] = cleaned_sentences\n",
    "\n",
    "    return cleaned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:52:59.113617Z",
     "iopub.status.busy": "2025-04-09T19:52:59.113307Z",
     "iopub.status.idle": "2025-04-09T19:52:59.212229Z",
     "shell.execute_reply": "2025-04-09T19:52:59.211184Z",
     "shell.execute_reply.started": "2025-04-09T19:52:59.113595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "my_dict = clean_and_join_sentences(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:53:49.700343Z",
     "iopub.status.busy": "2025-04-09T19:53:49.699960Z",
     "iopub.status.idle": "2025-04-09T19:53:49.718223Z",
     "shell.execute_reply": "2025-04-09T19:53:49.717181Z",
     "shell.execute_reply.started": "2025-04-09T19:53:49.700315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(my_dict)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T20:20:31.751105Z",
     "iopub.status.busy": "2025-04-09T20:20:31.750570Z",
     "iopub.status.idle": "2025-04-09T20:20:31.757141Z",
     "shell.execute_reply": "2025-04-09T20:20:31.755826Z",
     "shell.execute_reply.started": "2025-04-09T20:20:31.751068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Corpus BLEU score: 0.3466\n"
     ]
    }
   ],
   "source": [
    "def compute_bleu(references, hypotheses):\n",
    "    references = [[ref.split()] for ref in references]  \n",
    "    hypotheses = [hyp.split() for hyp in hypotheses]    \n",
    "\n",
    "    smoothie = SmoothingFunction().method4 \n",
    "\n",
    "    score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "    return score\n",
    "\n",
    "bleu_score = compute_bleu(\n",
    "    my_dict[\"original_translation\"],\n",
    "    my_dict[\"produced_translation\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Corpus BLEU score: {bleu_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 592212,
     "sourceId": 1067156,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7098601,
     "sourceId": 11345197,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 297573,
     "modelInstanceId": 276684,
     "sourceId": 329771,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
